Supervised Learning
	Data set contains right answers
	Choose more "right answers"
	Regression problem - predict continuous value
		How to choose whether to use linear regression or quadratic?
	Classification problem - predict discrete value (e.g., 0 or 1)
		Feature - attribute
		Example: breast tumor -> malignant or benign
			size -> classification
			size/age -> classification
				find linear formula that separates two sets
		Possible to use an infinite number of features
			Support Vector Machine?
Unsupervised Learning
	Data isn't labeled
	Try to find some structure in the data
		Clustering of items
			e.g. Google News
			e.g. gene expressions
			e.g. social network analysis
			e.g. market segmentation
				Don't know whawt the market segments are in advance
			e.g. astronomical analysis?
		Cocktail Party Problem
			two speakers, two microphones => separates audio streams
			one line of code in Octave
				linear algebra

Linear Regression with One Variable
	Model Representation
		House Sizes and Prices
			Predict price from size
		Training Set
			m = number of training examples
			x's = input variables, features - independent variables
			y's = output variables, target variables - dependent variables
			(x,y) = one training example
			(x^i, y^i) - ith training example
		Feed Training to Learning Algorithm
		Learning Algorithm outputs a function h (hypothesis) that maps x's to y's
		h_θ(x) = θ₀ + θ₁x
		univariate linear regression - one variable, x
	Cost Function
		θ_i's - parameters
		How to fit the best possible straight line to our data
			How to choose parameters
				Choose θ₀, θ₁ so h(x) is close to y for our training examples (x,y)
				Solve minimization problem
					We want (h(x) - y)^2 to be small
					squared error function: 1/(2m)∑(h_θ(x^i)-y^i)^2
		Contour plots - 2d representation of 3d plot
			all points on elipsis have same value of J(θ₀,θ₁)
	Gradient Descent
		Algorithm for minimizing cost function
		minimize J(θ₀,θ₁)
			Start with some θ₀,θ₁, e.g. θ₀ = 0, θ₁ = 0
			Keep changing to reduce J(θ₀,θ₁)
		Starting on hill, take steps downhill until you hit a local minimum
			different starting points will give you different local minimum
		a := b - assignment
		a = b - truth assertion
		α - learning rate
		simultaneously update θ₀ and θ₁
			Example:
			temp1 = θ₀ + √(θ₀θ₁)
			temp2 = θ₁ + √(θ₀θ₁)
			temp1 = 1 + √2
			temp2 = 2 + √2
			θ₀ = 1 + √2
			θ₁ = 2 + √2
